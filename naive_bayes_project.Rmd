---
title: "naive_bayes_project"
author: "Manuel Herrera Lara y Anahí Berumen Murillo"
date: "21/11/2020"
output: pdf_document
---

## Perform a basic data analysis describing the dataset, summary statistics, data distribution, etc.
### The data domain
We are going to apply the naive bayes algorithm to a dataset that contains comments or tweets directed towards the president of the United States, Donald Trump. In order to classify tweets
as positive or negative; and thus achieve to have in context the opinions of the people towards the president. The tweets published by President Donald Trump are related to the 2020 presidential election process in the United States, and which is currently taking place. In this election process the candidates for the presidency are Donald Trump and Joe Biden; winning as president-elect Joe Biden. And finally we must emphasize that the algorithm to use "naive bayes" is used to classify everything that is text and is based on probability.

```{r fig.width=4, fig.height=40, fig.align='center', echo=FALSE, include=TRUE}
library(png)
library(jpeg)
library(grid)
img <- readPNG("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/imagenes/tweet_dt1.png")
grid.raster(img)
```

\newpage
## How the data was recollected, limitations of the study, disadvantages, etc.
The data was collected from the social network Twitter and from these tweets we formed a dataset with 204 observations. The tweets are related to the United States election process, in which Joe Biden appeared as president-elect and Donald Trump alleges fraud.

## Description of the variables of the dataset
type
: Indicates the classification of the tweet. (P = Positivo y N = Negativo)

tweet
: Contains the text of the comment

## >> (dataset reading)
```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
# path of the dataset
setwd("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/")

# read the dataset
tweets_de_trump <- read.csv("tweets_donald_trump.csv", stringsAsFactors = FALSE)
```

## Basic summary statics
- It shows the first 10 records of the dataset.
```{r, include=TRUE}
head(tweets_de_trump, 10)
```

\newpage
-  It shows the structure of the data and/or the data types of the attributes.\
```{r, include=TRUE}
str(tweets_de_trump)
```
##  Describe the distribution of the data.
### Exploring the Variables
#### Tweets
```{r, include=TRUE}
table(tweets_de_trump$type)
```

#### Percentage of tweets
```{r, include=TRUE}
tweets_table <- table(tweets_de_trump$type)
tweets_pct <- prop.table(tweets_table) * 100
round(tweets_pct, digits = 1)
```

## Application of the naive bayes algorithm
### step 2: Exploring and preparing the data
```{r, include=TRUE}

# dataset path
setwd("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/")

# dataset reading
tweets_de_trump <- read.csv("tweets_donald_trump.csv")
str(tweets_de_trump)
```
\newpage
## We transform the type element to factor
```{r, include=TRUE}
tweets_de_trump$type <- factor(tweets_de_trump$type)
str(tweets_de_trump)
```

```{r, include=TRUE}
table(tweets_de_trump$type)
```
# DATA PROCESSING STAGE
# Cleaning and standardization of text data
# - we remove the numbers and punctuation
# - we remove uninteresting words like and, but and or
# - we decompose the sentences into individual words
## package tm => remove numbers, punctuacion, uninteresting words as and, but and or, etc
```{r, include=TRUE}
# install.packages("tm")
library(tm)
```

## Corpus: Creation of text documents
```{r, include=TRUE}
# we create a corpus
tweets_corpus <- VCorpus( VectorSource(tweets_de_trump$tweet))
# we examine the corpus (Text document)
print(tweets_corpus)
```
\newpage
## we get a summary of the corpus
```{r, include=TRUE}
inspect(tweets_corpus[ 1: 4])
```
## we see a single document
```{r, include=TRUE}
as.character(tweets_corpus[[1]])
```
## we see multiple documents with lapply()
```{r, include=TRUE}
lapply(tweets_corpus[ 1: 2], as.character)
```
\newpage
## we clean the data
### we transform all tweets to lowercase with the function tolower()
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus, content_transformer(tolower))

## we compare the two corpus
as.character(tweets_corpus[[3]])
as.character((tweets_corpus_clean[[3]]))
```

## now we delete the numbers of the tweets
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeNumbers)
```

## we eliminate stop words(uninteresting words like and, but and or)
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeWords, stopwords())
```

## we remove the punctuation
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removePunctuation)
```

## we define a function that replaces the punctuation
```{r, include=TRUE}
replacePunctuacion <- function( x){
  gsub("[[: punct:]] +", " " , x)
}
```

## we do stemming (take words as playing, played and plays and transforms them to their base form => play)
```{r, include=TRUE}
# install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
```
## we apply the wordStem() function to all documents
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stemDocument)
as.character(tweets_corpus[[18]])
as.character((tweets_corpus_clean[[18]]))
```
## we remove blank spaces
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stripWhitespace)

# data already cleaned
lapply(tweets_corpus[ 1: 3], as.character)
lapply(tweets_corpus_clean[ 1: 3], as.character)
```

## DATA PREPARATION: Splitting text documents into words
### we create a matrix => Document Term Matrix (DTM)
### Each column represents a word and the intersection tells us if the word appears or not in the tweet
```{r, include=TRUE}
tweets_dtm <- DocumentTermMatrix(tweets_corpus_clean)
```

## example of how we can create the DTM and clean the data at the same time
```{r, include=TRUE}
tweets_dtm_2 <- DocumentTermMatrix(tweets_corpus, control = list(
  tolower =  TRUE,
  removeNumbers = TRUE,
  stopwords = TRUE,
  removePunctuation = TRUE,
  stemming = TRUE
))

tweets_dtm_2
```

## We´ll divide the data into two porcions: 75 percent for training and 25 percent for testing
```{r, include=TRUE}
# SEPARAMOS LOS DATOS, EN DATOS DE ENTRENAMIENTO Y DATOS DE PRUEBA
# 75% para entrenamiento y 25% para prueba
tweets_dtm_train <- tweets_dtm[ 1: 153, ]
tweets_dtm_test <- tweets_dtm[ 154: 204, ]
tweets_train_labels <- tweets_de_trump[ 1: 153, ]$type  # variable a predecir y usamos el dataset original
tweets_test_labels <- tweets_de_trump[ 154: 204, ]$type
```

## We review the proportion of our 2 datasets; the training and the test.
```{r, include=TRUE}
prop.table( table(tweets_train_labels))
```

```{r, include=TRUE}
prop.table(table(tweets_test_labels))
```

## we find frequent words
```{r, include=TRUE}
tweets_freq_words <- findFreqTerms( tweets_dtm_train, 5)
str(tweets_freq_words)
```

## we filter our frequent terms in our matrix with training data and test data.
### The white space indicates all lines.
```{r, include=TRUE}

tweets_dtm_freq_train <- tweets_dtm_train[ , tweets_freq_words]
tweets_dtm_freq_test <- tweets_dtm_test[ , tweets_freq_words]
```

## The Naive Bayes classifier needs categorical data
## function that converts values to categories
```{r, include=TRUE}
convert_counts <- function(x){x <- ifelse(x > 0, "Yes", "No")}
tweets_train <- apply(tweets_dtm_freq_train, MARGIN = 2, convert_counts) 
tweets_test <- apply(tweets_dtm_freq_test, MARGIN = 2, convert_counts)
```

\newpage
# >> APPLYING NAIVE BAYES
# Step 3.- Train a data model

```{r, include=TRUE}
#install.packages("e1071")
library("e1071")

# La función naiveBayes() solo espera los datos de entrenamiento y la clase.
# construimos un modelo

 # le pasamos los datos de entrenamiento y etiquetas de entrenamiento
tweets_classifier <- naiveBayes(tweets_train, tweets_train_labels)
```

# step 4.- Evaluate the performance of the model
# we evaluate the model with our test data
```{r, include=TRUE}
tweets_test_pred <- predict(tweets_classifier, tweets_test) # >> le pasamos el modelo y los datos de prueba
```

## now we compare the predictions with the actual values
### we compare what the algorithm said with reality
```{r, include=TRUE}
# install.packages("gmodels")
library(gmodels)
CrossTable( tweets_test_pred, tweets_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c(' predicted', 'actual'))
```

# SIMPLE OUT >> we only pass predictions and test labels
```{r, include=TRUE}
table(tweets_test_pred, tweets_test_labels)
```
# Step 5. Improving the performance model with laplace
```{r, include=TRUE}
tweets_classifier_2 <- naiveBayes(tweets_train, tweets_train_labels, laplace = 1)
tweets_test_pred_2 <- predict(tweets_classifier_2, tweets_test)

CrossTable(tweets_test_pred_2, tweets_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))

```
\newpage
```{r, include=TRUE}
table(tweets_test_pred, tweets_test_labels)
```

