library(tm)
```
## Corpus: Creation of text documents
```{r, include=TRUE}
# we create a corpus
tweets_corpus <- VCorpus( VectorSource(tweets_de_trump$tweet))
# we examine the corpus (Text document)
print(tweets_corpus)
```
## we get a summary of the corpus
```{r, include=TRUE}
inspect(tweets_corpus[ 1: 4])
```
## we see a single document
```{r, include=TRUE}
as.character(tweets_corpus[[1]])
```
## we see multiple documents with lapply()
```{r, include=TRUE}
lapply(tweets_corpus[ 1: 2], as.character)
```
## we clean the data
### we transform all tweets to lowercase with the function tolower()
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus, content_transformer(tolower))
## we compare the two corpus
as.character(tweets_corpus[[3]])
as.character((tweets_corpus_clean[[3]]))
```
## now we delete the numbers of the tweets
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeNumbers)
as.character(tweets_corpus[[7]])
as.character((tweets_corpus_clean[[7]]))
```
## we eliminate stop words(uninteresting words like and, but and or)
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeWords, stopwords())
as.character(tweets_corpus[[1]])
as.character((tweets_corpus_clean[[1]]))
```
## we remove the punctuation
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removePunctuation)
as.character(tweets_corpus[[23]])
as.character((tweets_corpus_clean[[23]]))
```
## we define a function that replaces the punctuation
```{r, include=TRUE}
replacePunctuacion <- function( x){
gsub("[[: punct:]] +", " " , x)
}
```
## we do stemming (take words as playing, played and plays and transforms them to their base form => play)
```{r, include=TRUE}
# install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
```
## we apply the wordStem () function to all documents
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stemDocument)
as.character(tweets_corpus[[18]])
as.character((tweets_corpus_clean[[18]]))
```
## we remove blank spaces
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stripWhitespace)
# data already cleaned
lapply(tweets_corpus[ 1: 3], as.character)
lapply(tweets_corpus_clean[ 1: 3], as.character)
```
## DATA PREPARATION: Splitting text documents into words
### we create a matrix => Document Term Matrix (DTM)
### Each column represents a word and the intersection tells us if the word appears or not in the tweet
```{r, include=TRUE}
tweets_dtm <- DocumentTermMatrix(tweets_corpus_clean)
```
## example of how we can create the DTM and clean the data at the same time
```{r, include=TRUE}
tweets_dtm_2 <- DocumentTermMatrix(tweets_corpus, control = list(
tolower =  TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
tweets_dtm_2
```
## We´ll divide the data into two porcions: 75 percent for training and 25 percent for testing
```{r, include=TRUE}
tweets_dtm_train <- tweets_dtm[ 1: 153, ]
tweets_dtm_test <- tweets_dtm[ 154: 204, ]
tweets_train_labels <- tweets_de_trump[ 1: 153, ]$type
tweets_test_labels <- tweets_de_trump[ 154: 204, ]$type
```
## We review the proportion of our 2 datasets; the training and the test.
```{r, include=TRUE}
prop.table( table(tweets_train_labels))
```
```{r, include=TRUE}
prop.table(table(tweets_test_labels))
```
---
title: "naive_bayes_project"
author: "Manuel Herrera Lara y Anahí Berumen Murillo"
date: "21/11/2020"
output: pdf_document
---
## Perform a basic data analysis describing the dataset, summary statistics, data distribution, etc.
### The data domain
Vamos aplicar el algoritmo naive bayes a un dataset que contiene comentarios o tweets dirigidos
hacia el presidente de los Estados Unidos, Donald Trump. Con la finalidad de clasificar los tweets
como positivos o negativos; y asi lograr tener en contexto las opiniones de las personas hacia el presidente.
Los tweets publicados por el presidente Donald Trump estan relacionados con el proceso de elecciones
presidenciales de 2020 en Estados Unidos, y que se esta llevando actualmente. En este proceso de elecciones
los candidatos a la presidencia son Donald Trump y Joe Biden; ganando como presidente electo Joe Biden.
Y por último debemos destacar que el algoritmo a emplear "naive bayes" se utiliza para clasificar todo lo que sea texto
y esta basado en probabilidad.
```{r fig.width=4, fig.height=40, fig.align='center', echo=FALSE, include=TRUE}
library(png)
library(jpeg)
library(grid)
img <- readPNG("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/imagenes/tweet_dt1.png")
grid.raster(img)
```
\newpage
## How the data was recollected, limitations of the study, disadvantages, etc.
Los datos fueron recolectados de la red social Twitter y apartir de estos tweets
formamos un dataset con 204 observaciones. Los tweets estan relacionados con el proceso
de elecciones de Estados Unidos, en el que Joe Biden apareció como presidente electo y Donald Trump alega fraude.
## Description of the variables of the dataset
type
: Indica la clasficación del tweet. (P = Positivo y N = Negativo)
tweet
: Contiene el texto del comentario
## >> (dataset reading)
```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
# path of the dataset
setwd("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/")
# read the dataset
tweets_de_trump <- read.csv("tweets_donald_trump.csv", stringsAsFactors = FALSE)
```
## Basic summary statics
- It shows the first 10 records of the dataset.
```{r, include=TRUE}
head(tweets_de_trump, 10)
```
\newpage
-  It shows the structure of the data and/or the data types of the attributes.\
```{r, include=TRUE}
str(tweets_de_trump)
```
##  Describe the distribution of the data.
### Exploring the Variables
#### Tweets
```{r, include=TRUE}
table(tweets_de_trump$type)
```
#### Percentage of tweets
```{r, include=TRUE}
tweets_table <- table(tweets_de_trump$type)
tweets_pct <- prop.table(tweets_table) * 100
round(tweets_pct, digits = 1)
```
## Application of the naive bayes algorithm
### step 2: Exploring and preparing the data
```{r, include=TRUE}
# we read the dataset
setwd("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/")
tweets_de_trump <- read.csv("tweets_donald_trump.csv")
str(tweets_de_trump)
```
## We transform the type element to factor
```{r, include=TRUE}
tweets_de_trump$type <- factor(tweets_de_trump$type)
str(tweets_de_trump)
```
```{r, include=TRUE}
table(tweets_de_trump$type)
```
## pachage tm => remove numbers, punctuacion, uninteresting words as and, but and or, etc
```{r, include=TRUE}
# install.packages("tm")
library(tm)
```
## Corpus: Creation of text documents
```{r, include=TRUE}
# we create a corpus
tweets_corpus <- VCorpus( VectorSource(tweets_de_trump$tweet))
# we examine the corpus (Text document)
print(tweets_corpus)
```
## we get a summary of the corpus
```{r, include=TRUE}
inspect(tweets_corpus[ 1: 4])
```
## we see a single document
```{r, include=TRUE}
as.character(tweets_corpus[[1]])
```
## we see multiple documents with lapply()
```{r, include=TRUE}
lapply(tweets_corpus[ 1: 2], as.character)
```
## we clean the data
### we transform all tweets to lowercase with the function tolower()
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus, content_transformer(tolower))
## we compare the two corpus
as.character(tweets_corpus[[3]])
as.character((tweets_corpus_clean[[3]]))
```
## now we delete the numbers of the tweets
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeNumbers)
as.character(tweets_corpus[[7]])
as.character((tweets_corpus_clean[[7]]))
```
## we eliminate stop words(uninteresting words like and, but and or)
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeWords, stopwords())
as.character(tweets_corpus[[1]])
as.character((tweets_corpus_clean[[1]]))
```
## we remove the punctuation
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removePunctuation)
as.character(tweets_corpus[[23]])
as.character((tweets_corpus_clean[[23]]))
```
## we define a function that replaces the punctuation
```{r, include=TRUE}
replacePunctuacion <- function( x){
gsub("[[: punct:]] +", " " , x)
}
```
## we do stemming (take words as playing, played and plays and transforms them to their base form => play)
```{r, include=TRUE}
# install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
```
## we apply the wordStem () function to all documents
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stemDocument)
as.character(tweets_corpus[[18]])
as.character((tweets_corpus_clean[[18]]))
```
## we remove blank spaces
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stripWhitespace)
# data already cleaned
lapply(tweets_corpus[ 1: 3], as.character)
lapply(tweets_corpus_clean[ 1: 3], as.character)
```
## DATA PREPARATION: Splitting text documents into words
### we create a matrix => Document Term Matrix (DTM)
### Each column represents a word and the intersection tells us if the word appears or not in the tweet
```{r, include=TRUE}
tweets_dtm <- DocumentTermMatrix(tweets_corpus_clean)
```
## example of how we can create the DTM and clean the data at the same time
```{r, include=TRUE}
tweets_dtm_2 <- DocumentTermMatrix(tweets_corpus, control = list(
tolower =  TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
tweets_dtm_2
```
## We´ll divide the data into two porcions: 75 percent for training and 25 percent for testing
```{r, include=TRUE}
tweets_dtm_train <- tweets_dtm[ 1: 153, ]
tweets_dtm_test <- tweets_dtm[ 154: 204, ]
tweets_train_labels <- tweets_de_trump[ 1: 153, ]$type
tweets_test_labels <- tweets_de_trump[ 154: 204, ]$type
```
## We review the proportion of our 2 datasets; the training and the test.
```{r, include=TRUE}
prop.table( table(tweets_train_labels))
```
```{r, include=TRUE}
prop.table(table(tweets_test_labels))
```
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud::wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
library(wordcloud)
wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
prop.table( table(tweets_train_labels))
prop.table( table(tweets_train_labels))
prop.table( table(tweets_train_labels))
prop.table(table(tweets_test_labels))
library(png)
library(jpeg)
library(grid)
img <- readPNG("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/imagenes/tweet_dt1.png")
grid.raster(img)
library(png)
library(jpeg)
library(grid)
img <- readPNG("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/imagenes/tweet_dt1.png")
grid.raster(img)
knitr::opts_chunk$set(echo = TRUE)
# path of the dataset
setwd("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/")
# read the dataset
tweets_de_trump <- read.csv("tweets_donald_trump.csv", stringsAsFactors = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# path of the dataset
setwd("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/")
# read the dataset
tweets_de_trump <- read.csv("tweets_donald_trump.csv", stringsAsFactors = FALSE)
head(tweets_de_trump, 10)
str(tweets_de_trump)
str(tweets_de_trump)
table(tweets_de_trump$type)
tweets_table <- table(tweets_de_trump$type)
tweets_pct <- prop.table(tweets_table) * 100
round(tweets_pct, digits = 1)
# we read the dataset
setwd("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/")
tweets_de_trump <- read.csv("tweets_donald_trump.csv")
str(tweets_de_trump)
tweets_de_trump$type <- factor(tweets_de_trump$type)
str(tweets_de_trump)
# we read the dataset
setwd("/home/chino/Documentos/17_materias_IS/1_mineria_de_datos/10_semana_miniproyecto3/1_algoritmo_naive_bayes/")
tweets_de_trump <- read.csv("tweets_donald_trump.csv")
str(tweets_de_trump)
tweets_de_trump$type <- factor(tweets_de_trump$type)
str(tweets_de_trump)
table(tweets_de_trump$type)
# install.packages("tm")
library(tm)
# install.packages("tm")
library(tm)
# install.packages("tm")
library(tm)
# we create a corpus
tweets_corpus <- VCorpus( VectorSource(tweets_de_trump$tweet))
# we examine the corpus (Text document)
print(tweets_corpus)
inspect(tweets_corpus[ 1: 4])
# we create a corpus
tweets_corpus <- VCorpus( VectorSource(tweets_de_trump$tweet))
# we examine the corpus (Text document)
print(tweets_corpus)
inspect(tweets_corpus[ 1: 4])
as.character(tweets_corpus[[1]])
lapply(tweets_corpus[ 1: 2], as.character)
tweets_corpus_clean <- tm::tm_map(tweets_corpus, content_transformer(tolower))
## we compare the two corpus
as.character(tweets_corpus[[3]])
as.character((tweets_corpus_clean[[3]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus, content_transformer(tolower))
## we compare the two corpus
as.character(tweets_corpus[[3]])
as.character((tweets_corpus_clean[[3]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeNumbers)
as.character(tweets_corpus[[7]])
as.character((tweets_corpus_clean[[7]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeWords, stopwords())
as.character(tweets_corpus[[1]])
as.character((tweets_corpus_clean[[1]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeNumbers)
as.character(tweets_corpus[[7]])
as.character((tweets_corpus_clean[[7]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removeWords, stopwords())
as.character(tweets_corpus[[1]])
as.character((tweets_corpus_clean[[1]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, removePunctuation)
as.character(tweets_corpus[[23]])
as.character((tweets_corpus_clean[[23]]))
# install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stemDocument)
as.character(tweets_corpus[[18]])
as.character((tweets_corpus_clean[[18]]))
## we do stemming (take words as playing, played and plays and transforms them to their base form => play)
```{r, include=TRUE}
# install.packages("SnowballC")
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
```
## we apply the wordStem() function to all documents
```{r, include=TRUE}
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stemDocument)
as.character(tweets_corpus[[18]])
as.character((tweets_corpus_clean[[18]]))
```
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stemDocument)
as.character(tweets_corpus[[18]])
as.character((tweets_corpus_clean[[18]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stemDocument)
as.character(tweets_corpus[[18]])
as.character((tweets_corpus_clean[[18]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stemDocument)
as.character(tweets_corpus[[18]])
as.character((tweets_corpus_clean[[18]]))
tweets_corpus_clean <- tm::tm_map(tweets_corpus_clean, stripWhitespace)
# data already cleaned
lapply(tweets_corpus[ 1: 3], as.character)
lapply(tweets_corpus_clean[ 1: 3], as.character)
tweets_dtm <- DocumentTermMatrix(tweets_corpus_clean)
tweets_dtm_2 <- DocumentTermMatrix(tweets_corpus, control = list(
tolower =  TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
tweets_dtm_2
tweets_dtm_train <- tweets_dtm[ 1: 153, ]
tweets_dtm_test <- tweets_dtm[ 154: 204, ]
tweets_train_labels <- tweets_de_trump[ 1: 153, ]$type
tweets_test_labels <- tweets_de_trump[ 154: 204, ]$type
tweets_dtm_train <- tweets_dtm[ 1: 153, ]
tweets_dtm_test <- tweets_dtm[ 154: 204, ]
tweets_train_labels <- tweets_de_trump[ 1: 153, ]$type
tweets_test_labels <- tweets_de_trump[ 154: 204, ]$type
prop.table( table(tweets_train_labels))
prop.table(table(tweets_test_labels))
prop.table( table(tweets_train_labels))
library(wordcloud)
wordcloud(tweets_corpus_clean, min.freq = 50, random.order = FALSE)
prop.table( table(tweets_train_labels))
prop.table(table(tweets_test_labels))
tweets_freq_words <- findFreqTerms( tweets_dtm_train, 5)
str(tweets_freq_words)
tweets_dtm_freq_train <- tweets_dtm_train[ , tweets_freq_words]
tweets_dtm_freq_test <- tweets_dtm_test[ , tweets_freq_words]
convert_counts <- function(x){x <- ifelse(x > 0, "Yes", "No")}
tweets_train <- apply(tweets_dtm_freq_train, MARGIN = 2, convert_counts)
tweets_test <- apply(tweets_dtm_freq_test, MARGIN = 2, convert_counts)
tweets_dtm_freq_train <- tweets_dtm_train[ , tweets_freq_words]
tweets_dtm_freq_test <- tweets_dtm_test[ , tweets_freq_words]
convert_counts <- function(x){x <- ifelse(x > 0, "Yes", "No")}
tweets_train <- apply(tweets_dtm_freq_train, MARGIN = 2, convert_counts)
tweets_test <- apply(tweets_dtm_freq_test, MARGIN = 2, convert_counts)
#install.packages("e1071")
library("e1071")
#install.packages("e1071")
library("e1071")
#install.packages("e1071")
library("e1071")
# La función naiveBayes() solo espera los datos de entrenamiento y la clase.
# construimos un modelo
# le pasamos los datos de entrenamiento y etiquetas de entrenamiento
tweets_classifier <- naiveBayes(tweets_train, tweets_train_labels)
tweets_test_pred <- predict(tweets_classifier, tweets_test) # >> le pasamos el modelo y los datos de prueba
install.packages("gmodels")
library(gmodels)
CrossTa
#install.packages("e1071")
library("e1071")
# La función naiveBayes() solo espera los datos de entrenamiento y la clase.
# construimos un modelo
# le pasamos los datos de entrenamiento y etiquetas de entrenamiento
tweets_classifier <- naiveBayes(tweets_train, tweets_train_labels)
#install.packages("e1071")
library("e1071")
# La función naiveBayes() solo espera los datos de entrenamiento y la clase.
# construimos un modelo
# le pasamos los datos de entrenamiento y etiquetas de entrenamiento
tweets_classifier <- naiveBayes(tweets_train, tweets_train_labels)
tweets_test_pred <- predict(tweets_classifier, tweets_test) # >> le pasamos el modelo y los datos de prueba
# install.packages("gmodels")
library(gmodels)
CrossTable( tweets_test_pred, tweets_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c(' predicted', 'actual'))
table(tweets_test_pred, tweets_test_labels)
tweets_classifier_2 <- naiveBayes(tweets_train, tweets_train_labels, laplace = 1)
tweets_test_pred_2 <- predict(tweets_classifier_2, tweets_test)
tweets_classifier_2 <- naiveBayes(tweets_train, tweets_train_labels, laplace = 1)
tweets_test_pred_2 <- predict(tweets_classifier_2, tweets_test)
CrossTable(tweets_test_pred_2, tweets_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))
tweets_classifier_2 <- naiveBayes(tweets_train, tweets_train_labels, laplace = 1)
tweets_test_pred_2 <- predict(tweets_classifier_2, tweets_test)
CrossTable(tweets_test_pred_2, tweets_test_labels, prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE, dnn = c('predicted', 'actual'))
table(tweets_test_pred, tweets_test_labels)
# install.packages("gmodels")
library(gmodels)
CrossTable( tweets_test_pred, tweets_test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c(' predicted', 'actual'))
table(tweets_test_pred, tweets_test_labels)
